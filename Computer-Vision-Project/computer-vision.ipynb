{"cells":[{"cell_type":"markdown","metadata":{"id":"PiIoXQU1bpWq"},"source":["# Notebook Imports"]},{"cell_type":"code","execution_count":129,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T15:53:54.567559Z","iopub.status.busy":"2024-02-16T15:53:54.566739Z","iopub.status.idle":"2024-02-16T15:54:34.512435Z","shell.execute_reply":"2024-02-16T15:54:34.511230Z","shell.execute_reply.started":"2024-02-16T15:53:54.567520Z"},"id":"0E_HT4OxbpWv","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: ultralytics in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (8.1.6)\n","Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ultralytics) (0.16.2)\n","Requirement already satisfied: py-cpuinfo in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: numpy>=1.22.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ultralytics) (1.25.2)\n","Requirement already satisfied: pillow>=7.1.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ultralytics) (10.2.0)\n","Requirement already satisfied: thop>=0.1.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ultralytics) (0.1.1.post2209072238)\n","Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ultralytics) (4.9.0.80)\n","Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ultralytics) (3.8.2)\n","Requirement already satisfied: pandas>=1.1.4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ultralytics) (2.2.0)\n","Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ultralytics) (0.13.2)\n","Requirement already satisfied: scipy>=1.4.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ultralytics) (1.12.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ultralytics) (6.0.1)\n","Requirement already satisfied: psutil in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from ultralytics) (5.9.8)\n","Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ultralytics) (4.66.1)\n","Requirement already satisfied: requests>=2.23.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ultralytics) (2.31.0)\n","Requirement already satisfied: torch>=1.8.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ultralytics) (2.1.2)\n","Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n","Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n","Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.47.2)\n","Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n","Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2023.4)\n","Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2023.11.17)\n","Requirement already satisfied: networkx in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n","Requirement already satisfied: typing-extensions in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.9.0)\n","Requirement already satisfied: jinja2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.3)\n","Requirement already satisfied: filelock in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n","Requirement already satisfied: fsspec in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2023.12.2)\n","Requirement already satisfied: sympy in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.12)\n","Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n","Requirement already satisfied: six>=1.5 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.4)\n","Requirement already satisfied: mpmath>=0.19 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip available: 22.2.1 -> 24.0\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]},{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["ERROR: Could not find a version that satisfies the requirement cv2 (from versions: none)\n","ERROR: No matching distribution found for cv2\n","\n","[notice] A new release of pip available: 22.2.1 -> 24.0\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tensorflow in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.16.1)\n","Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (2.16.1)\n","Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n","Requirement already satisfied: setuptools in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (63.2.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.9.0)\n","Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n","Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n","Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.25.2)\n","Requirement already satisfied: keras>=3.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.2.1)\n","Requirement already satisfied: packaging in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (23.2)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.62.2)\n","Requirement already satisfied: libclang>=13.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n","Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n","Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n","Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n","Requirement already satisfied: h5py>=3.10.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.11.0)\n","Requirement already satisfied: six>=1.12.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.25.3)\n","Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n","Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.43.0)\n","Requirement already satisfied: rich in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.7.1)\n","Requirement already satisfied: namex in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.8)\n","Requirement already satisfied: optree in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.11.0)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2023.11.17)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.3.2)\n","Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.6)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.0.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.4)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.17.2)\n","Requirement already satisfied: mdurl~=0.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.2)\n","Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip available: 22.2.1 -> 24.0\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: haversine in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.8.1)\n","Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip available: 22.2.1 -> 24.0\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]}],"source":["%pip install ultralytics\n","%pip install cv2\n","%pip install tensorflow\n","%pip install haversine"]},{"cell_type":"code","execution_count":130,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T15:54:34.514909Z","iopub.status.busy":"2024-02-16T15:54:34.514580Z","iopub.status.idle":"2024-02-16T15:54:41.913763Z","shell.execute_reply":"2024-02-16T15:54:41.912970Z","shell.execute_reply.started":"2024-02-16T15:54:34.514877Z"},"executionInfo":{"elapsed":13212,"status":"ok","timestamp":1703700042652,"user":{"displayName":"Zukhrakhon Gulomova","userId":"14559221766365366106"},"user_tz":-300},"id":"U6asmcevbpWx","trusted":true},"outputs":[],"source":["from ultralytics import YOLO, solutions\n","from ultralytics.solutions import speed_estimation\n","from collections import defaultdict\n","\n","import haversine as hs   \n","from haversine import Unit\n","\n","import torch\n","import torchvision.transforms as T\n","from torchvision.transforms import functional as F\n","from torchvision.models.detection import ssd300_vgg16, fasterrcnn_resnet50_fpn_v2\n","\n","import numpy as np\n","import cv2\n","import math\n","import time\n","import json\n","import os"]},{"cell_type":"markdown","metadata":{"id":"dwU9E-PcbpWy"},"source":["## Video Details"]},{"cell_type":"code","execution_count":131,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T15:58:43.655566Z","iopub.status.busy":"2024-02-16T15:58:43.654442Z","iopub.status.idle":"2024-02-16T15:58:43.659769Z","shell.execute_reply":"2024-02-16T15:58:43.658747Z","shell.execute_reply.started":"2024-02-16T15:58:43.655535Z"},"executionInfo":{"elapsed":333,"status":"ok","timestamp":1703700047664,"user":{"displayName":"Zukhrakhon Gulomova","userId":"14559221766365366106"},"user_tz":-300},"id":"FXNcwqC1bpWy","trusted":true},"outputs":[],"source":["IMAGE_PATH = \"seattle13.jpg\"\n","VIDEO_PATH = \"highway.mp4\"\n","\n","SPEED_LIMIT = 70"]},{"cell_type":"markdown","metadata":{},"source":["## COCO Names"]},{"cell_type":"code","execution_count":132,"metadata":{},"outputs":[],"source":["COCO_INSTANCE_CATEGORY_NAMES = [\n","    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n","    'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n","    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n","    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',\n","    'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n","    'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n","    'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork',\n","    'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n","    'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n","    'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'TV', 'laptop',\n","    'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven',\n","    'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n","    'teddy bear', 'hair drier', 'toothbrush'\n","]"]},{"cell_type":"markdown","metadata":{},"source":["## Loading the Data from JSON file"]},{"cell_type":"code","execution_count":133,"metadata":{},"outputs":[],"source":["with open('points.json') as file:\n","    file_contents = json.load(file)"]},{"cell_type":"markdown","metadata":{"id":"bslqA7DebpWy"},"source":["## Main Functions of Speed Estimation"]},{"cell_type":"markdown","metadata":{"id":"MSLRStNvbpWz"},"source":["### Euclidean Distance"]},{"cell_type":"code","execution_count":134,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T15:54:41.925620Z","iopub.status.busy":"2024-02-16T15:54:41.925356Z","iopub.status.idle":"2024-02-16T15:54:41.946723Z","shell.execute_reply":"2024-02-16T15:54:41.945879Z","shell.execute_reply.started":"2024-02-16T15:54:41.925597Z"},"executionInfo":{"elapsed":301,"status":"ok","timestamp":1703700050976,"user":{"displayName":"Zukhrakhon Gulomova","userId":"14559221766365366106"},"user_tz":-300},"id":"n1NVEFG8bpWz","trusted":true},"outputs":[],"source":["def euclidean_distance(point_a: tuple, point_b: tuple) -> float:\n","\n","    \"\"\"\n","    Calculate the Euclidean distance between two points.\n","\n","    Parameters:\n","    - point_a (tuple): (x, y) coordinates of the first point.\n","    - point_b (tuple): (x, y) coordinates of the second point.\n","\n","    Returns:\n","    float: Euclidean distance between the two points.\n","    \"\"\"\n","\n","    x_a, y_a = point_a\n","    x_b, y_b = point_b\n","    distance = round(math.sqrt((x_b - x_a) ** 2 + (y_b - y_a) ** 2), 3)\n","    return distance"]},{"cell_type":"markdown","metadata":{},"source":["### Scaling Factor"]},{"cell_type":"code","execution_count":135,"metadata":{},"outputs":[],"source":["def scaling_factor(dict_file: dict) -> float :\n","\n","    \"\"\"\n","    Calculate the scaling factor between real-world distances and pixel distances.\n","\n","    This function computes a scaling factor that converts distances measured in pixels\n","    to distances measured in real-world units (meters). The input dictionary is expected \n","    to contain coordinates in both real-world (latitude and longitude) and pixel formats.\n","\n","    Parameters:\n","        dict_file (dict): A dictionary with the following keys:\n","            - 'pts': A list of two tuples, each containing latitude and longitude coordinates.\n","            - '2dpts': A list of two tuples, each containing x and y pixel coordinates.\n","\n","    Returns:\n","        float: The scaling factor representing the ratio of the real-world distance \n","               (in meters) to the pixel distance.\n","\n","    Example:\n","        >>> dict_file = {\n","                'pts': [(37.7749, -122.4194), (34.0522, -118.2437)],\n","                '2dpts': [(100, 200), (300, 400)]\n","            }\n","        >>> scaling_factor(dict_file)\n","        0.45\n","    \"\"\"\n","\n","    coordinate_1 = tuple(dict_file['pts'][0])\n","    coordinate_2 = tuple(dict_file['pts'][1])\n","    \n","    REAL_WORLD_DISTANCE = round(hs.haversine(coordinate_1, coordinate_2, unit=Unit.METERS))\n","    X = tuple(dict_file['2dpts'][0])\n","    Y = tuple(dict_file['2dpts'][1])\n","\n","    PIXEL_DISTANCE = round(euclidean_distance(X, Y))\n","    SCALING_FACTOR = round(REAL_WORLD_DISTANCE /  PIXEL_DISTANCE, 2)\n","    return SCALING_FACTOR"]},{"cell_type":"code","execution_count":136,"metadata":{},"outputs":[{"data":{"text/plain":["0.33"]},"execution_count":136,"metadata":{},"output_type":"execute_result"}],"source":["SCALING_FACTOR = scaling_factor(dict_file=file_contents)\n","SCALING_FACTOR"]},{"cell_type":"markdown","metadata":{"id":"z1zNHveTbpW0"},"source":["### Calculate Speed"]},{"cell_type":"code","execution_count":153,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T15:54:41.948490Z","iopub.status.busy":"2024-02-16T15:54:41.948053Z","iopub.status.idle":"2024-02-16T15:54:41.956924Z","shell.execute_reply":"2024-02-16T15:54:41.956058Z","shell.execute_reply.started":"2024-02-16T15:54:41.948459Z"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1703700056528,"user":{"displayName":"Zukhrakhon Gulomova","userId":"14559221766365366106"},"user_tz":-300},"id":"BQv3tUTkbpW0","trusted":true},"outputs":[],"source":["def calculate_speed(current_center:tuple, previous_center:tuple, scaling_factor:float, \n","                    time_interval:float, frame, points:tuple):\n","\n","    \"\"\"\n","    Calculate and display the speed of a moving object in the video frame.\n","\n","    Parameters:\n","    - current_center (tuple): (x, y) coordinates of the current center of the car.\n","    - previous_center (tuple): (x, y) coordinates of the previous center of the car.\n","    - scaling_factor (float): Scaling factor to convert pixel distance to real-world distance.\n","    - time_interval (float): Time interval between frames.\n","    - frame: Current video frame.\n","    - points (tuple): (x, y) coordinates for placing the speed text in the frame.\n","    \"\"\"\n","\n","    distance_pixels = euclidean_distance(current_center, previous_center)\n","    distance_meters = distance_pixels * scaling_factor\n","    speed = round(distance_meters / time_interval * 3.6)\n","        \n","    speed_text = f\"Speed: {speed} km/h\"\n","    color = (0, 0, 250) if speed > SPEED_LIMIT else (0, 0, 0)\n","\n","    cv2.putText(frame, speed_text, points, cv2.FONT_HERSHEY_SIMPLEX, 1.5, color, 3)"]},{"cell_type":"markdown","metadata":{"id":"y8wvRCdebpW1"},"source":["## YOLOv8"]},{"cell_type":"code","execution_count":138,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","0: 384x640 2 cars, 109.0ms\n","Speed: 4.0ms preprocess, 109.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 174.5ms\n","Speed: 4.0ms preprocess, 174.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 104.2ms\n","Speed: 5.5ms preprocess, 104.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 101.5ms\n","Speed: 2.0ms preprocess, 101.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 114.0ms\n","Speed: 2.3ms preprocess, 114.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 104.0ms\n","Speed: 5.4ms preprocess, 104.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 131.0ms\n","Speed: 3.2ms preprocess, 131.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 133.7ms\n","Speed: 2.5ms preprocess, 133.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 104.1ms\n","Speed: 2.0ms preprocess, 104.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 124.2ms\n","Speed: 6.0ms preprocess, 124.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 113.8ms\n","Speed: 3.0ms preprocess, 113.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 208.0ms\n","Speed: 4.0ms preprocess, 208.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 219.0ms\n","Speed: 4.0ms preprocess, 219.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 229.0ms\n","Speed: 4.0ms preprocess, 229.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 192.8ms\n","Speed: 7.0ms preprocess, 192.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 218.0ms\n","Speed: 5.0ms preprocess, 218.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 219.0ms\n","Speed: 4.0ms preprocess, 219.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 242.9ms\n","Speed: 7.0ms preprocess, 242.9ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 221.9ms\n","Speed: 7.0ms preprocess, 221.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 228.0ms\n","Speed: 8.0ms preprocess, 228.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 292.5ms\n","Speed: 4.0ms preprocess, 292.5ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 989.0ms\n","Speed: 112.7ms preprocess, 989.0ms inference, 27.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 416.2ms\n","Speed: 5.0ms preprocess, 416.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 368.5ms\n","Speed: 5.2ms preprocess, 368.5ms inference, 9.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 446.9ms\n","Speed: 17.0ms preprocess, 446.9ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 971.8ms\n","Speed: 14.1ms preprocess, 971.8ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 883.8ms\n","Speed: 6.6ms preprocess, 883.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 230.0ms\n","Speed: 16.0ms preprocess, 230.0ms inference, 8.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 155.7ms\n","Speed: 4.0ms preprocess, 155.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 373.8ms\n","Speed: 3.3ms preprocess, 373.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 400.3ms\n","Speed: 7.8ms preprocess, 400.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 362.1ms\n","Speed: 3.0ms preprocess, 362.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 319.0ms\n","Speed: 3.1ms preprocess, 319.0ms inference, 10.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 473.6ms\n","Speed: 3.5ms preprocess, 473.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 159.9ms\n","Speed: 3.8ms preprocess, 159.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 244.4ms\n","Speed: 2.0ms preprocess, 244.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 464.0ms\n","Speed: 2.8ms preprocess, 464.0ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 338.0ms\n","Speed: 12.0ms preprocess, 338.0ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 195.2ms\n","Speed: 4.3ms preprocess, 195.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 868.1ms\n","Speed: 3.1ms preprocess, 868.1ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 230.4ms\n","Speed: 7.0ms preprocess, 230.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 208.0ms\n","Speed: 5.0ms preprocess, 208.0ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 284.4ms\n","Speed: 8.0ms preprocess, 284.4ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 378.0ms\n","Speed: 4.0ms preprocess, 378.0ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 161.7ms\n","Speed: 3.5ms preprocess, 161.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 240.0ms\n","Speed: 5.0ms preprocess, 240.0ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 501.0ms\n","Speed: 16.0ms preprocess, 501.0ms inference, 12.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1083.9ms\n","Speed: 34.0ms preprocess, 1083.9ms inference, 33.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1204.2ms\n","Speed: 9.0ms preprocess, 1204.2ms inference, 14.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 332.9ms\n","Speed: 5.2ms preprocess, 332.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 314.6ms\n","Speed: 6.1ms preprocess, 314.6ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 305.2ms\n","Speed: 6.1ms preprocess, 305.2ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 431.7ms\n","Speed: 11.0ms preprocess, 431.7ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 361.5ms\n","Speed: 5.3ms preprocess, 361.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 743.3ms\n","Speed: 12.3ms preprocess, 743.3ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 686.4ms\n","Speed: 25.9ms preprocess, 686.4ms inference, 15.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 2348.0ms\n","Speed: 9.1ms preprocess, 2348.0ms inference, 6.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 536.0ms\n","Speed: 17.3ms preprocess, 536.0ms inference, 13.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 785.5ms\n","Speed: 344.7ms preprocess, 785.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 306.5ms\n","Speed: 5.6ms preprocess, 306.5ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 235.9ms\n","Speed: 7.0ms preprocess, 235.9ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 250.7ms\n","Speed: 6.0ms preprocess, 250.7ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 156.7ms\n","Speed: 3.3ms preprocess, 156.7ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 116.2ms\n","Speed: 3.0ms preprocess, 116.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 218.3ms\n","Speed: 4.4ms preprocess, 218.3ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 142.4ms\n","Speed: 3.0ms preprocess, 142.4ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 211.1ms\n","Speed: 5.9ms preprocess, 211.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 332.2ms\n","Speed: 7.0ms preprocess, 332.2ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 235.1ms\n","Speed: 6.0ms preprocess, 235.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 160.7ms\n","Speed: 4.0ms preprocess, 160.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 229.4ms\n","Speed: 3.5ms preprocess, 229.4ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 212.3ms\n","Speed: 5.0ms preprocess, 212.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 175.7ms\n","Speed: 6.0ms preprocess, 175.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 644.5ms\n","Speed: 3.2ms preprocess, 644.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 129.6ms\n","Speed: 3.2ms preprocess, 129.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 109.2ms\n","Speed: 3.5ms preprocess, 109.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 295.9ms\n","Speed: 40.0ms preprocess, 295.9ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 134.0ms\n","Speed: 3.4ms preprocess, 134.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 130.6ms\n","Speed: 3.0ms preprocess, 130.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 131.0ms\n","Speed: 4.0ms preprocess, 131.0ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 243.9ms\n","Speed: 78.4ms preprocess, 243.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 166.3ms\n","Speed: 6.0ms preprocess, 166.3ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 156.6ms\n","Speed: 4.0ms preprocess, 156.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 151.1ms\n","Speed: 3.0ms preprocess, 151.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 102.4ms\n","Speed: 2.0ms preprocess, 102.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 123.3ms\n","Speed: 5.0ms preprocess, 123.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 134.6ms\n","Speed: 3.1ms preprocess, 134.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 115.8ms\n","Speed: 3.2ms preprocess, 115.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 127.2ms\n","Speed: 3.0ms preprocess, 127.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 122.4ms\n","Speed: 5.0ms preprocess, 122.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 144.3ms\n","Speed: 3.0ms preprocess, 144.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 141.1ms\n","Speed: 4.1ms preprocess, 141.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 160.4ms\n","Speed: 3.1ms preprocess, 160.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 142.6ms\n","Speed: 3.0ms preprocess, 142.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 144.3ms\n","Speed: 3.2ms preprocess, 144.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 118.0ms\n","Speed: 2.4ms preprocess, 118.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 121.0ms\n","Speed: 6.3ms preprocess, 121.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 134.3ms\n","Speed: 3.9ms preprocess, 134.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 134.1ms\n","Speed: 2.2ms preprocess, 134.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 121.8ms\n","Speed: 3.0ms preprocess, 121.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 137.0ms\n","Speed: 7.0ms preprocess, 137.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 133.6ms\n","Speed: 4.2ms preprocess, 133.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 135.3ms\n","Speed: 4.5ms preprocess, 135.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 128.4ms\n","Speed: 3.0ms preprocess, 128.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 113.4ms\n","Speed: 5.0ms preprocess, 113.4ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 165.5ms\n","Speed: 3.2ms preprocess, 165.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 134.0ms\n","Speed: 3.0ms preprocess, 134.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 244.4ms\n","Speed: 42.0ms preprocess, 244.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 134.5ms\n","Speed: 3.0ms preprocess, 134.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 132.6ms\n","Speed: 4.1ms preprocess, 132.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 124.1ms\n","Speed: 3.5ms preprocess, 124.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 147.6ms\n","Speed: 3.3ms preprocess, 147.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 142.8ms\n","Speed: 4.0ms preprocess, 142.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 140.7ms\n","Speed: 3.9ms preprocess, 140.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 125.0ms\n","Speed: 3.1ms preprocess, 125.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 152.0ms\n","Speed: 3.3ms preprocess, 152.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 147.0ms\n","Speed: 3.2ms preprocess, 147.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 126.1ms\n","Speed: 3.4ms preprocess, 126.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 125.9ms\n","Speed: 3.4ms preprocess, 125.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 144.9ms\n","Speed: 3.6ms preprocess, 144.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 143.5ms\n","Speed: 11.9ms preprocess, 143.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 123.6ms\n","Speed: 3.3ms preprocess, 123.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 268.9ms\n","Speed: 5.0ms preprocess, 268.9ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 139.8ms\n","Speed: 3.3ms preprocess, 139.8ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 129.0ms\n","Speed: 6.3ms preprocess, 129.0ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 125.0ms\n","Speed: 3.2ms preprocess, 125.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 127.4ms\n","Speed: 3.1ms preprocess, 127.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 181.1ms\n","Speed: 3.7ms preprocess, 181.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 163.9ms\n","Speed: 3.1ms preprocess, 163.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 161.9ms\n","Speed: 3.3ms preprocess, 161.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 401.5ms\n","Speed: 6.7ms preprocess, 401.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 130.0ms\n","Speed: 5.8ms preprocess, 130.0ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 135.0ms\n","Speed: 6.1ms preprocess, 135.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 127.0ms\n","Speed: 4.7ms preprocess, 127.0ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 146.0ms\n","Speed: 4.1ms preprocess, 146.0ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 131.9ms\n","Speed: 2.5ms preprocess, 131.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 124.8ms\n","Speed: 2.3ms preprocess, 124.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 140.6ms\n","Speed: 3.1ms preprocess, 140.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 124.6ms\n","Speed: 6.0ms preprocess, 124.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 118.6ms\n","Speed: 2.5ms preprocess, 118.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 133.7ms\n","Speed: 2.4ms preprocess, 133.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 160.2ms\n","Speed: 2.1ms preprocess, 160.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 156.3ms\n","Speed: 3.2ms preprocess, 156.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 142.5ms\n","Speed: 7.0ms preprocess, 142.5ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 131.4ms\n","Speed: 7.0ms preprocess, 131.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 129.0ms\n","Speed: 3.0ms preprocess, 129.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 129.9ms\n","Speed: 3.0ms preprocess, 129.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 156.4ms\n","Speed: 3.0ms preprocess, 156.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 125.0ms\n","Speed: 2.0ms preprocess, 125.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 129.0ms\n","Speed: 3.0ms preprocess, 129.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 141.1ms\n","Speed: 3.0ms preprocess, 141.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 111.4ms\n","Speed: 3.0ms preprocess, 111.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 125.1ms\n","Speed: 3.0ms preprocess, 125.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 126.0ms\n","Speed: 3.0ms preprocess, 126.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n"]}],"source":["output = cv2.VideoWriter(\"yolov8.mp4\", cv2.VideoWriter_fourcc(*'DIVX'), 30, (1920, 1080))\n","model = YOLO(\"yolov8n.pt\")\n","\n","cap = cv2.VideoCapture(VIDEO_PATH)\n","if not cap.isOpened():\n","    print(\"Error opening video stream or file\")\n","    exit()\n","\n","num_frames = 0\n","start_time = time.time()\n","\n","while(cap.isOpened()):\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","\n","    num_frames += 1\n","\n","    # YOLOv8 object tracking\n","    predictions_yolo = model.track(frame, persist=True, classes=2, show_labels=False, show_conf=False,\n","                            tracker=\"bytetrack.yaml\")\n","\n","    # Visualize the results on the frame\n","    annotated_frame = predictions_yolo[0].plot()\n","\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","    YOLO_FPS = num_frames / elapsed_time\n","\n","    cv2.imshow(\"YOLOv8 Tracking\", annotated_frame)\n","    output.write(annotated_frame)\n","\n","    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n","        break\n","\n","cap.release()\n","output.release()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{},"source":["## Faster R-CNN "]},{"cell_type":"code","execution_count":139,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"data":{"text/plain":["FasterRCNN(\n","  (transform): GeneralizedRCNNTransform(\n","      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n","  )\n","  (backbone): BackboneWithFPN(\n","    (body): IntermediateLayerGetter(\n","      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (layer1): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer2): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer3): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (4): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (5): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer4): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","    )\n","    (fpn): FeaturePyramidNetwork(\n","      (inner_blocks): ModuleList(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (2): Conv2dNormActivation(\n","          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (3): Conv2dNormActivation(\n","          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (layer_blocks): ModuleList(\n","        (0-3): 4 x Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (extra_blocks): LastLevelMaxPool()\n","    )\n","  )\n","  (rpn): RegionProposalNetwork(\n","    (anchor_generator): AnchorGenerator()\n","    (head): RPNHead(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (1): ReLU(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (1): ReLU(inplace=True)\n","        )\n","      )\n","      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n","  (roi_heads): RoIHeads(\n","    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n","    (box_head): FastRCNNConvFCHead(\n","      (0): Conv2dNormActivation(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","      )\n","      (1): Conv2dNormActivation(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","      )\n","      (2): Conv2dNormActivation(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","      )\n","      (3): Conv2dNormActivation(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","      )\n","      (4): Flatten(start_dim=1, end_dim=-1)\n","      (5): Linear(in_features=12544, out_features=1024, bias=True)\n","      (6): ReLU(inplace=True)\n","    )\n","    (box_predictor): FastRCNNPredictor(\n","      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n","      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n","    )\n","  )\n",")"]},"execution_count":139,"metadata":{},"output_type":"execute_result"}],"source":["# Load a pre-trained Faster R-CNN model\n","model_02 = fasterrcnn_resnet50_fpn_v2(pretrained=True, progress=False)\n","model_02.eval()  # Set the model to inference mode"]},{"cell_type":"code","execution_count":140,"metadata":{},"outputs":[],"source":["# Function to load frame\n","def load_frame(frame):\n","    frame_tensor = F.to_tensor(frame)\n","    return frame_tensor"]},{"cell_type":"code","execution_count":141,"metadata":{},"outputs":[],"source":["# Function to visualize predictions on a frame using OpenCV\n","def visualize_predictions(frame, prediction, threshold=0.6):\n","    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","    frame_tensor = F.to_tensor(frame_rgb)\n","    frame_np = (frame_tensor.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n","    frame_bgr = cv2.cvtColor(frame_np, cv2.COLOR_RGB2BGR)\n","\n","    boxes = prediction[0]['boxes'].cpu().numpy()\n","    scores = prediction[0]['scores'].cpu().numpy()\n","    labels = prediction[0]['labels'].cpu().numpy()\n","\n","    car_boxes = []\n","    \n","    for box, score, label in zip(boxes, scores, labels):\n","        if score > threshold and label < len(COCO_INSTANCE_CATEGORY_NAMES):\n","            if COCO_INSTANCE_CATEGORY_NAMES[label] == 'car':\n","                try:\n","                    x_min, y_min, x_max, y_max = map(int, box)\n","                    cv2.rectangle(frame_bgr, (x_min, y_min), (x_max, y_max), (0, 165, 255), 2) \n","                    cv2.putText(frame_bgr, f'{COCO_INSTANCE_CATEGORY_NAMES[label]}: {score:.2f}', \n","                                (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 165, 255), 2)\n","                    car_boxes.append(box)\n","                except Exception as e:\n","                    print(f\"Error drawing rectangle or text: {e}\")\n","    \n","    return frame_bgr, car_boxes"]},{"cell_type":"code","execution_count":142,"metadata":{},"outputs":[],"source":["# Open the video file\n","cap = cv2.VideoCapture(VIDEO_PATH)\n","\n","# Check if video opened successfully\n","if not cap.isOpened():\n","    print(\"Error: Could not open video.\")\n","    exit()\n","\n","# Get video writer to save the annotated video\n","output = cv2.VideoWriter('faster-rcnn.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 30, (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), \n","                                                                                   int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n","\n","num_frames = 0\n","start_time = time.time()\n","\n","# Process each frame\n","while cap.isOpened():\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","\n","    frame_tensor = load_frame(frame)\n","\n","    # Make predictions\n","    with torch.no_grad():\n","        prediction_faster_rcnn = model_02([frame_tensor])\n","\n","    # Visualize predictions\n","    annotated_frame, car_boxes_f = visualize_predictions(frame, prediction_faster_rcnn)\n","\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","    FASTER_RCNN_FPS = num_frames / elapsed_time\n","\n","    # Display the annotated frame\n","    cv2.imshow('Predictions', annotated_frame)\n","\n","    # Write the annotated frame to the output video\n","    output.write(annotated_frame)\n","\n","    # Press 'q' to exit the video early\n","    if cv2.waitKey(1) & 0xFF == ord('q'):\n","        break\n","\n","# Release the video capture and writer objects\n","cap.release()\n","output.release()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{},"source":["## SSD"]},{"cell_type":"code","execution_count":143,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SSD300_VGG16_Weights.COCO_V1`. You can also use `weights=SSD300_VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"data":{"text/plain":["SSD(\n","  (backbone): SSDFeatureExtractorVGG(\n","    (features): Sequential(\n","      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (6): ReLU(inplace=True)\n","      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (8): ReLU(inplace=True)\n","      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (11): ReLU(inplace=True)\n","      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (13): ReLU(inplace=True)\n","      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (15): ReLU(inplace=True)\n","      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n","      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (18): ReLU(inplace=True)\n","      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (20): ReLU(inplace=True)\n","      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (22): ReLU(inplace=True)\n","    )\n","    (extra): ModuleList(\n","      (0): Sequential(\n","        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","        (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (4): ReLU(inplace=True)\n","        (5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (6): ReLU(inplace=True)\n","        (7): Sequential(\n","          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n","          (1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n","          (2): ReLU(inplace=True)\n","          (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n","          (4): ReLU(inplace=True)\n","        )\n","      )\n","      (1): Sequential(\n","        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        (3): ReLU(inplace=True)\n","      )\n","      (2): Sequential(\n","        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        (3): ReLU(inplace=True)\n","      )\n","      (3-4): 2 x Sequential(\n","        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n","        (3): ReLU(inplace=True)\n","      )\n","    )\n","  )\n","  (anchor_generator): DefaultBoxGenerator(aspect_ratios=[[2], [2, 3], [2, 3], [2, 3], [2], [2]], clip=True, scales=[0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05], steps=[8, 16, 32, 64, 100, 300])\n","  (head): SSDHead(\n","    (classification_head): SSDClassificationHead(\n","      (module_list): ModuleList(\n","        (0): Conv2d(512, 364, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): Conv2d(1024, 546, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (2): Conv2d(512, 546, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (3): Conv2d(256, 546, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (4-5): 2 x Conv2d(256, 364, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (regression_head): SSDRegressionHead(\n","      (module_list): ModuleList(\n","        (0): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): Conv2d(1024, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (2): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (3): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (4-5): 2 x Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","  )\n","  (transform): GeneralizedRCNNTransform(\n","      Normalize(mean=[0.48235, 0.45882, 0.40784], std=[0.00392156862745098, 0.00392156862745098, 0.00392156862745098])\n","      Resize(min_size=(300,), max_size=300, mode='bilinear')\n","  )\n",")"]},"execution_count":143,"metadata":{},"output_type":"execute_result"}],"source":["model_03 = ssd300_vgg16(pretrained=True)\n","model_03.eval()"]},{"cell_type":"code","execution_count":144,"metadata":{},"outputs":[],"source":["# Open the video file\n","cap = cv2.VideoCapture(VIDEO_PATH)\n","\n","# Check if video opened successfully\n","if not cap.isOpened():\n","    print(\"Error: Could not open video.\")\n","    exit()\n","\n","# Get video writer to save the annotated video\n","output = cv2.VideoWriter('ssd.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 30, (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), \n","                                                                          int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n","\n","num_frames = 0\n","start_time = time.time()\n","\n","# Process each frame\n","while cap.isOpened():\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","\n","    frame_tensor = load_frame(frame)\n","\n","    # Make predictions\n","    with torch.no_grad():\n","        prediction_ssd = model_03([frame_tensor])\n","\n","    # Visualize predictions\n","    annotated_frame, car_boxes_s = visualize_predictions(frame, prediction_ssd, threshold=0.5)\n","\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","    SSD_FPS = num_frames / elapsed_time\n","\n","    # Display the annotated frame\n","    cv2.imshow('Predictions', annotated_frame)\n","\n","    # Write the annotated frame to the output video\n","    output.write(annotated_frame)\n","\n","    # Press 'q' to exit the video early\n","    if cv2.waitKey(1) & 0xFF == ord('q'):\n","        break\n","\n","# Release the video capture and writer objects\n","cap.release()\n","output.release()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{},"source":["## Comparison"]},{"cell_type":"markdown","metadata":{},"source":["### Model Size\n","\n","* **YOLOv8:** 12 MB\n","* **Faster RCNN:** 167 MB\n","* **SSD:** 136 MB"]},{"cell_type":"code","execution_count":145,"metadata":{},"outputs":[],"source":["def get_model_size(model, model_path):\n","    torch.save(model.state_dict(), model_path)\n","    model_size = os.path.getsize(model_path)\n","    model_size = round(model_size / (1024 * 1024))\n","    return model_size"]},{"cell_type":"code","execution_count":146,"metadata":{},"outputs":[],"source":["YOLO_MODEL_SIZE = get_model_size(model=model, model_path='yolo.pth')\n","FASTER_RCNN_MODEL_SIZE = get_model_size(model=model_02, model_path='fasterrcnn.pth')\n","SSD_MODEL_SIZE = get_model_size(model=model_03, model_path='ssd.pth')"]},{"cell_type":"code","execution_count":147,"metadata":{},"outputs":[{"data":{"text/plain":["(12, 167, 136)"]},"execution_count":147,"metadata":{},"output_type":"execute_result"}],"source":["YOLO_MODEL_SIZE, FASTER_RCNN_MODEL_SIZE, SSD_MODEL_SIZE"]},{"cell_type":"markdown","metadata":{},"source":["### Speed: FPS\n","\n","* **YOLOv8:** 4 FPS\n","* **Faster RCNN:** 0.1 FPS\n","* **SSD:** 2 FPS\n","\n","I am using CPU, so it is much slower than GPU. Your video processing speed might be faster than mine."]},{"cell_type":"code","execution_count":148,"metadata":{},"outputs":[{"data":{"text/plain":["(2.9513940976633943, 0.0, 0.0)"]},"execution_count":148,"metadata":{},"output_type":"execute_result"}],"source":["YOLO_FPS, FASTER_RCNN_FPS, SSD_FPS"]},{"cell_type":"markdown","metadata":{},"source":["### Total Detected Cars\n","\n","* **YOLOv8:** 4 cars\n","* **Faster RCNN:** 11 cars\n","* **SSD:** 1 car"]},{"cell_type":"code","execution_count":149,"metadata":{},"outputs":[{"data":{"text/plain":["(4, 14, 0)"]},"execution_count":149,"metadata":{},"output_type":"execute_result"}],"source":["number_of_cars_yolo = len(predictions_yolo[0].boxes)\n","number_of_cars_faster_rcnn = len(car_boxes_f)\n","number_of_cars_ssd = len(car_boxes_s)\n","\n","number_of_cars_yolo, number_of_cars_faster_rcnn, number_of_cars_ssd"]},{"cell_type":"markdown","metadata":{},"source":["## Speed Estimation with YOLOv8"]},{"cell_type":"markdown","metadata":{},"source":["### My Speed Estimation Function"]},{"cell_type":"code","execution_count":154,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","0: 384x640 2 cars, 150.8ms\n","Speed: 25.3ms preprocess, 150.8ms inference, 6.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 178.5ms\n","Speed: 5.0ms preprocess, 178.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 108.7ms\n","Speed: 2.4ms preprocess, 108.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 106.1ms\n","Speed: 3.0ms preprocess, 106.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 106.3ms\n","Speed: 4.0ms preprocess, 106.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 106.9ms\n","Speed: 2.3ms preprocess, 106.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 131.2ms\n","Speed: 2.0ms preprocess, 131.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 103.0ms\n","Speed: 2.0ms preprocess, 103.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 104.9ms\n","Speed: 4.0ms preprocess, 104.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 100.4ms\n","Speed: 4.1ms preprocess, 100.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 94.6ms\n","Speed: 5.0ms preprocess, 94.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 93.0ms\n","Speed: 3.2ms preprocess, 93.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 185.0ms\n","Speed: 3.2ms preprocess, 185.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 159.9ms\n","Speed: 3.0ms preprocess, 159.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 95.0ms\n","Speed: 4.3ms preprocess, 95.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 96.4ms\n","Speed: 2.2ms preprocess, 96.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 91.5ms\n","Speed: 3.4ms preprocess, 91.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 96.6ms\n","Speed: 4.0ms preprocess, 96.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 101.3ms\n","Speed: 3.2ms preprocess, 101.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 91.2ms\n","Speed: 4.1ms preprocess, 91.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 91.3ms\n","Speed: 4.0ms preprocess, 91.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 92.5ms\n","Speed: 3.0ms preprocess, 92.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 89.9ms\n","Speed: 4.0ms preprocess, 89.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 105.0ms\n","Speed: 3.3ms preprocess, 105.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 95.8ms\n","Speed: 4.0ms preprocess, 95.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 99.7ms\n","Speed: 4.0ms preprocess, 99.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 107.3ms\n","Speed: 3.4ms preprocess, 107.3ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 122.8ms\n","Speed: 5.0ms preprocess, 122.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 123.6ms\n","Speed: 6.0ms preprocess, 123.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 115.2ms\n","Speed: 3.3ms preprocess, 115.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 104.9ms\n","Speed: 2.2ms preprocess, 104.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 121.4ms\n","Speed: 3.6ms preprocess, 121.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 168.5ms\n","Speed: 3.4ms preprocess, 168.5ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 150.7ms\n","Speed: 4.0ms preprocess, 150.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 120.4ms\n","Speed: 3.4ms preprocess, 120.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 115.1ms\n","Speed: 3.2ms preprocess, 115.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 121.5ms\n","Speed: 4.4ms preprocess, 121.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 111.3ms\n","Speed: 2.0ms preprocess, 111.3ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 127.4ms\n","Speed: 4.3ms preprocess, 127.4ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 113.0ms\n","Speed: 3.2ms preprocess, 113.0ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 109.0ms\n","Speed: 4.2ms preprocess, 109.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 218.0ms\n","Speed: 5.0ms preprocess, 218.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 130.6ms\n","Speed: 4.1ms preprocess, 130.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 126.8ms\n","Speed: 5.0ms preprocess, 126.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 109.0ms\n","Speed: 4.2ms preprocess, 109.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 128.6ms\n","Speed: 3.1ms preprocess, 128.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 128.8ms\n","Speed: 3.0ms preprocess, 128.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 113.7ms\n","Speed: 5.0ms preprocess, 113.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 125.6ms\n","Speed: 4.4ms preprocess, 125.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 114.2ms\n","Speed: 2.7ms preprocess, 114.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 124.4ms\n","Speed: 5.0ms preprocess, 124.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 110.1ms\n","Speed: 3.5ms preprocess, 110.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 113.6ms\n","Speed: 2.4ms preprocess, 113.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 119.2ms\n","Speed: 4.6ms preprocess, 119.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 123.4ms\n","Speed: 4.1ms preprocess, 123.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 109.6ms\n","Speed: 3.7ms preprocess, 109.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 112.6ms\n","Speed: 4.0ms preprocess, 112.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 112.0ms\n","Speed: 5.0ms preprocess, 112.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 124.4ms\n","Speed: 4.4ms preprocess, 124.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 115.8ms\n","Speed: 5.0ms preprocess, 115.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 111.4ms\n","Speed: 3.7ms preprocess, 111.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 119.5ms\n","Speed: 4.1ms preprocess, 119.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 118.3ms\n","Speed: 4.3ms preprocess, 118.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 116.0ms\n","Speed: 4.0ms preprocess, 116.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 107.8ms\n","Speed: 3.4ms preprocess, 107.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 117.7ms\n","Speed: 4.4ms preprocess, 117.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 116.2ms\n","Speed: 4.3ms preprocess, 116.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 218.0ms\n","Speed: 4.0ms preprocess, 218.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 120.3ms\n","Speed: 5.0ms preprocess, 120.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 123.8ms\n","Speed: 5.2ms preprocess, 123.8ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 119.9ms\n","Speed: 4.4ms preprocess, 119.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 118.9ms\n","Speed: 2.5ms preprocess, 118.9ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 104.0ms\n","Speed: 3.2ms preprocess, 104.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 104.9ms\n","Speed: 5.0ms preprocess, 104.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 104.4ms\n","Speed: 3.6ms preprocess, 104.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 126.0ms\n","Speed: 4.2ms preprocess, 126.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 121.1ms\n","Speed: 3.2ms preprocess, 121.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 112.7ms\n","Speed: 4.0ms preprocess, 112.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 126.7ms\n","Speed: 5.0ms preprocess, 126.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 106.0ms\n","Speed: 3.5ms preprocess, 106.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 106.7ms\n","Speed: 4.0ms preprocess, 106.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 159.7ms\n","Speed: 3.0ms preprocess, 159.7ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 163.8ms\n","Speed: 5.0ms preprocess, 163.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 158.4ms\n","Speed: 3.0ms preprocess, 158.4ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 136.8ms\n","Speed: 3.5ms preprocess, 136.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 110.2ms\n","Speed: 5.3ms preprocess, 110.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 111.7ms\n","Speed: 4.4ms preprocess, 111.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 123.8ms\n","Speed: 2.6ms preprocess, 123.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 114.5ms\n","Speed: 4.3ms preprocess, 114.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 178.2ms\n","Speed: 5.0ms preprocess, 178.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 162.5ms\n","Speed: 4.0ms preprocess, 162.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 190.0ms\n","Speed: 4.1ms preprocess, 190.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 196.5ms\n","Speed: 6.0ms preprocess, 196.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 205.9ms\n","Speed: 4.0ms preprocess, 205.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 230.0ms\n","Speed: 5.0ms preprocess, 230.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 231.0ms\n","Speed: 7.0ms preprocess, 231.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 209.2ms\n","Speed: 5.0ms preprocess, 209.2ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 238.5ms\n","Speed: 6.0ms preprocess, 238.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 228.0ms\n","Speed: 6.0ms preprocess, 228.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 229.0ms\n","Speed: 5.0ms preprocess, 229.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 259.0ms\n","Speed: 10.0ms preprocess, 259.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 247.0ms\n","Speed: 8.0ms preprocess, 247.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 207.0ms\n","Speed: 11.0ms preprocess, 207.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 227.8ms\n","Speed: 4.0ms preprocess, 227.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 216.3ms\n","Speed: 7.0ms preprocess, 216.3ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 202.4ms\n","Speed: 6.0ms preprocess, 202.4ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 185.2ms\n","Speed: 4.0ms preprocess, 185.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 199.6ms\n","Speed: 4.0ms preprocess, 199.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 213.5ms\n","Speed: 7.0ms preprocess, 213.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 138.5ms\n","Speed: 5.3ms preprocess, 138.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 136.0ms\n","Speed: 4.3ms preprocess, 136.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 135.5ms\n","Speed: 3.5ms preprocess, 135.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 132.9ms\n","Speed: 6.0ms preprocess, 132.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 118.4ms\n","Speed: 3.8ms preprocess, 118.4ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 123.3ms\n","Speed: 2.5ms preprocess, 123.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 125.8ms\n","Speed: 2.4ms preprocess, 125.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 121.4ms\n","Speed: 4.0ms preprocess, 121.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 126.8ms\n","Speed: 3.5ms preprocess, 126.8ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 121.1ms\n","Speed: 3.1ms preprocess, 121.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 118.8ms\n","Speed: 4.1ms preprocess, 118.8ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 134.5ms\n","Speed: 3.1ms preprocess, 134.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 116.9ms\n","Speed: 2.1ms preprocess, 116.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 117.5ms\n","Speed: 2.0ms preprocess, 117.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 210.0ms\n","Speed: 3.0ms preprocess, 210.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 123.6ms\n","Speed: 4.0ms preprocess, 123.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 114.1ms\n","Speed: 2.0ms preprocess, 114.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 140.4ms\n","Speed: 4.5ms preprocess, 140.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 127.8ms\n","Speed: 3.4ms preprocess, 127.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 146.5ms\n","Speed: 3.1ms preprocess, 146.5ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 135.4ms\n","Speed: 4.1ms preprocess, 135.4ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 106.4ms\n","Speed: 2.4ms preprocess, 106.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 150.5ms\n","Speed: 7.2ms preprocess, 150.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 110.4ms\n","Speed: 3.4ms preprocess, 110.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 121.9ms\n","Speed: 2.0ms preprocess, 121.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 126.9ms\n","Speed: 3.4ms preprocess, 126.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 121.3ms\n","Speed: 3.4ms preprocess, 121.3ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 117.4ms\n","Speed: 3.3ms preprocess, 117.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 111.2ms\n","Speed: 3.1ms preprocess, 111.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 175.7ms\n","Speed: 3.1ms preprocess, 175.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 107.5ms\n","Speed: 4.2ms preprocess, 107.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 99.3ms\n","Speed: 3.1ms preprocess, 99.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 106.2ms\n","Speed: 4.5ms preprocess, 106.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 107.9ms\n","Speed: 5.0ms preprocess, 107.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 118.6ms\n","Speed: 2.4ms preprocess, 118.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 116.2ms\n","Speed: 4.0ms preprocess, 116.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 121.1ms\n","Speed: 2.2ms preprocess, 121.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 108.4ms\n","Speed: 3.3ms preprocess, 108.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 126.4ms\n","Speed: 3.5ms preprocess, 126.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 108.5ms\n","Speed: 2.3ms preprocess, 108.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 112.6ms\n","Speed: 2.2ms preprocess, 112.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 219.1ms\n","Speed: 4.0ms preprocess, 219.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 108.5ms\n","Speed: 5.0ms preprocess, 108.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 114.5ms\n","Speed: 3.1ms preprocess, 114.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 113.7ms\n","Speed: 3.1ms preprocess, 113.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n"]}],"source":["output = cv2.VideoWriter(\"yolov8-speed-estimation.mp4\", cv2.VideoWriter_fourcc(*'DIVX'), 30, (1920, 1080))\n","model = YOLO(\"yolov8n.pt\")\n","\n","cap = cv2.VideoCapture(VIDEO_PATH)\n","if not cap.isOpened():\n","    print(\"Error opening video stream or file\")\n","    exit()\n","\n","num_frames = 0\n","start_time = time.time()\n","\n","track_history = defaultdict(list)\n","\n","while(cap.isOpened()):\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","\n","    num_frames += 1\n","    previous_frame_time = time.time()\n","\n","    # YOLOv8 object tracking\n","    results = model.track(frame, persist=True, classes=2, show_labels=False, show_conf=False,\n","                            tracker=\"bytetrack.yaml\")\n","\n","    # Get the boxes and track IDs\n","    boxes = results[0].boxes.xywh.cpu()\n","    track_ids = results[0].boxes.id.cpu().tolist()\n","\n","    # Visualize the results on the frame\n","    annotated_frame = results[0].plot()\n","\n","    for box, track_id in zip(boxes, track_ids):\n","        x, y, w, h = box\n","        track = track_history[track_id]\n","        track.append((float(x), float(y)))  # x, y center point\n","\n","    end_time = time.time()\n","    elapsed_time = previous_frame_time - start_time\n","    FPS = num_frames / elapsed_time\n","    time_interval = 1 / FPS \n","   \n","    for box, track_id in zip(boxes, track_ids):\n","        x, y, w, h = box\n","        if len(track_history[track_id]) > 1:\n","            calculate_speed(current_center=track_history[track_id][-1],\n","                            previous_center=track_history[track_id][-2],\n","                            scaling_factor=SCALING_FACTOR,\n","                            time_interval=time_interval,\n","                            frame=annotated_frame,\n","                            points=(int(x-(w/2)), int(y-(h/2) -10)))\n","\n","        # Draw the tracking lines\n","        points = np.hstack(track_history[track_id]).astype(np.int32).reshape((-1, 1, 2))\n","        cv2.polylines(annotated_frame, [points], isClosed=False,\n","                        color=(0, 0, 0), thickness=5)\n","\n","    cv2.imshow(\"YOLOv8 Tracking\", annotated_frame)\n","    output.write(annotated_frame)\n","\n","    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n","        break\n","\n","cap.release()\n","output.release()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{},"source":["### YOLOv8 pre-built Speed Estimation function"]},{"cell_type":"code","execution_count":152,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","0: 384x640 2 cars, 1 traffic light, 118.6ms\n","Speed: 3.1ms preprocess, 118.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 1 traffic light, 148.7ms\n","Speed: 7.0ms preprocess, 148.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 1 traffic light, 110.3ms\n","Speed: 3.3ms preprocess, 110.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 1 traffic light, 115.7ms\n","Speed: 4.0ms preprocess, 115.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 1 traffic light, 105.7ms\n","Speed: 5.0ms preprocess, 105.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 1 traffic light, 102.6ms\n","Speed: 3.1ms preprocess, 102.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 1 traffic light, 97.6ms\n","Speed: 2.2ms preprocess, 97.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 1 traffic light, 92.0ms\n","Speed: 2.5ms preprocess, 92.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 1 traffic light, 101.4ms\n","Speed: 1.0ms preprocess, 101.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 1 traffic light, 135.3ms\n","Speed: 2.4ms preprocess, 135.3ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 1 traffic light, 110.4ms\n","Speed: 3.0ms preprocess, 110.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 1 traffic light, 109.4ms\n","Speed: 1.1ms preprocess, 109.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 1 traffic light, 106.7ms\n","Speed: 2.4ms preprocess, 106.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 1 traffic light, 107.6ms\n","Speed: 4.0ms preprocess, 107.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 105.6ms\n","Speed: 2.7ms preprocess, 105.6ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 119.2ms\n","Speed: 1.4ms preprocess, 119.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 166.4ms\n","Speed: 4.4ms preprocess, 166.4ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 156.8ms\n","Speed: 5.2ms preprocess, 156.8ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 147.6ms\n","Speed: 3.0ms preprocess, 147.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 230.0ms\n","Speed: 3.0ms preprocess, 230.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 165.3ms\n","Speed: 3.0ms preprocess, 165.3ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 165.6ms\n","Speed: 4.0ms preprocess, 165.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 117.1ms\n","Speed: 3.4ms preprocess, 117.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 135.8ms\n","Speed: 4.0ms preprocess, 135.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 108.6ms\n","Speed: 5.0ms preprocess, 108.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 129.2ms\n","Speed: 2.0ms preprocess, 129.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 126.8ms\n","Speed: 2.6ms preprocess, 126.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 118.7ms\n","Speed: 3.0ms preprocess, 118.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 117.3ms\n","Speed: 2.3ms preprocess, 117.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 137.2ms\n","Speed: 3.0ms preprocess, 137.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 134.3ms\n","Speed: 2.4ms preprocess, 134.3ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 150.5ms\n","Speed: 4.0ms preprocess, 150.5ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 126.3ms\n","Speed: 3.2ms preprocess, 126.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 170.0ms\n","Speed: 3.0ms preprocess, 170.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 118.1ms\n","Speed: 3.0ms preprocess, 118.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 109.0ms\n","Speed: 2.5ms preprocess, 109.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 125.1ms\n","Speed: 3.1ms preprocess, 125.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 129.0ms\n","Speed: 4.0ms preprocess, 129.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 124.2ms\n","Speed: 4.0ms preprocess, 124.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 116.7ms\n","Speed: 4.0ms preprocess, 116.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 130.3ms\n","Speed: 2.0ms preprocess, 130.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 116.3ms\n","Speed: 3.0ms preprocess, 116.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 192.0ms\n","Speed: 3.0ms preprocess, 192.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 158.5ms\n","Speed: 4.0ms preprocess, 158.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 114.3ms\n","Speed: 3.0ms preprocess, 114.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 119.6ms\n","Speed: 2.5ms preprocess, 119.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 119.0ms\n","Speed: 3.0ms preprocess, 119.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 114.7ms\n","Speed: 3.4ms preprocess, 114.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 113.0ms\n","Speed: 4.1ms preprocess, 113.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 115.3ms\n","Speed: 1.1ms preprocess, 115.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 113.8ms\n","Speed: 3.5ms preprocess, 113.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 113.7ms\n","Speed: 3.6ms preprocess, 113.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 109.5ms\n","Speed: 3.4ms preprocess, 109.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 120.5ms\n","Speed: 3.1ms preprocess, 120.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 112.9ms\n","Speed: 3.0ms preprocess, 112.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 120.1ms\n","Speed: 2.2ms preprocess, 120.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 105.2ms\n","Speed: 3.4ms preprocess, 105.2ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 106.3ms\n","Speed: 4.0ms preprocess, 106.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 112.5ms\n","Speed: 3.3ms preprocess, 112.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 109.4ms\n","Speed: 2.7ms preprocess, 109.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 107.5ms\n","Speed: 3.4ms preprocess, 107.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 106.1ms\n","Speed: 2.3ms preprocess, 106.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 112.8ms\n","Speed: 3.0ms preprocess, 112.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 116.6ms\n","Speed: 5.0ms preprocess, 116.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 118.6ms\n","Speed: 3.0ms preprocess, 118.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 113.5ms\n","Speed: 2.2ms preprocess, 113.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 105.9ms\n","Speed: 2.3ms preprocess, 105.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 111.5ms\n","Speed: 5.0ms preprocess, 111.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 134.1ms\n","Speed: 2.4ms preprocess, 134.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 191.5ms\n","Speed: 3.2ms preprocess, 191.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 146.1ms\n","Speed: 5.0ms preprocess, 146.1ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 111.8ms\n","Speed: 3.6ms preprocess, 111.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 128.8ms\n","Speed: 3.4ms preprocess, 128.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 105.5ms\n","Speed: 2.0ms preprocess, 105.5ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 110.0ms\n","Speed: 4.3ms preprocess, 110.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 107.9ms\n","Speed: 3.2ms preprocess, 107.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 118.1ms\n","Speed: 3.2ms preprocess, 118.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 109.0ms\n","Speed: 3.2ms preprocess, 109.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 108.0ms\n","Speed: 4.9ms preprocess, 108.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 112.4ms\n","Speed: 3.0ms preprocess, 112.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 107.6ms\n","Speed: 2.6ms preprocess, 107.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 112.3ms\n","Speed: 2.2ms preprocess, 112.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 109.4ms\n","Speed: 3.0ms preprocess, 109.4ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 105.8ms\n","Speed: 4.0ms preprocess, 105.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 107.0ms\n","Speed: 2.3ms preprocess, 107.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 113.5ms\n","Speed: 3.0ms preprocess, 113.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 99.7ms\n","Speed: 2.1ms preprocess, 99.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 101.3ms\n","Speed: 2.1ms preprocess, 101.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 110.2ms\n","Speed: 4.0ms preprocess, 110.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 123.1ms\n","Speed: 4.0ms preprocess, 123.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 110.2ms\n","Speed: 3.2ms preprocess, 110.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 118.8ms\n","Speed: 3.9ms preprocess, 118.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 105.1ms\n","Speed: 3.3ms preprocess, 105.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 117.4ms\n","Speed: 3.0ms preprocess, 117.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 101.5ms\n","Speed: 3.0ms preprocess, 101.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 206.5ms\n","Speed: 4.0ms preprocess, 206.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 116.7ms\n","Speed: 5.2ms preprocess, 116.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 116.2ms\n","Speed: 2.4ms preprocess, 116.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 107.8ms\n","Speed: 3.7ms preprocess, 107.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 105.6ms\n","Speed: 3.6ms preprocess, 105.6ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 116.4ms\n","Speed: 4.1ms preprocess, 116.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 112.4ms\n","Speed: 4.7ms preprocess, 112.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 113.5ms\n","Speed: 4.0ms preprocess, 113.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 113.7ms\n","Speed: 2.5ms preprocess, 113.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 126.5ms\n","Speed: 3.1ms preprocess, 126.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 110.6ms\n","Speed: 3.3ms preprocess, 110.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 1 traffic light, 103.4ms\n","Speed: 3.0ms preprocess, 103.4ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 1 traffic light, 112.4ms\n","Speed: 2.0ms preprocess, 112.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 1 traffic light, 111.6ms\n","Speed: 3.0ms preprocess, 111.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 1 traffic light, 115.4ms\n","Speed: 2.2ms preprocess, 115.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 1 traffic light, 98.7ms\n","Speed: 2.8ms preprocess, 98.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 1 traffic light, 105.6ms\n","Speed: 2.1ms preprocess, 105.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 1 traffic light, 112.7ms\n","Speed: 4.4ms preprocess, 112.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 1 traffic light, 101.3ms\n","Speed: 3.3ms preprocess, 101.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 1 traffic light, 115.7ms\n","Speed: 3.0ms preprocess, 115.7ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 1 traffic light, 111.1ms\n","Speed: 2.4ms preprocess, 111.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 1 traffic light, 115.6ms\n","Speed: 3.2ms preprocess, 115.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 1 traffic light, 111.9ms\n","Speed: 2.1ms preprocess, 111.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 1 traffic light, 114.2ms\n","Speed: 3.4ms preprocess, 114.2ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 1 traffic light, 208.8ms\n","Speed: 5.3ms preprocess, 208.8ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 1 traffic light, 149.8ms\n","Speed: 4.0ms preprocess, 149.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 1 traffic light, 107.1ms\n","Speed: 3.0ms preprocess, 107.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 1 traffic light, 106.9ms\n","Speed: 2.4ms preprocess, 106.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 1 traffic light, 111.3ms\n","Speed: 2.4ms preprocess, 111.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 1 traffic light, 104.3ms\n","Speed: 3.0ms preprocess, 104.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 1 traffic light, 114.8ms\n","Speed: 3.0ms preprocess, 114.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 1 traffic light, 112.4ms\n","Speed: 3.6ms preprocess, 112.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 119.5ms\n","Speed: 2.0ms preprocess, 119.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 115.5ms\n","Speed: 3.0ms preprocess, 115.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 108.9ms\n","Speed: 3.2ms preprocess, 108.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 115.9ms\n","Speed: 4.0ms preprocess, 115.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 111.0ms\n","Speed: 2.0ms preprocess, 111.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 101.9ms\n","Speed: 4.0ms preprocess, 101.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 103.0ms\n","Speed: 2.1ms preprocess, 103.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 110.7ms\n","Speed: 3.4ms preprocess, 110.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 104.8ms\n","Speed: 3.0ms preprocess, 104.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 109.4ms\n","Speed: 3.6ms preprocess, 109.4ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 110.0ms\n","Speed: 3.3ms preprocess, 110.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 127.1ms\n","Speed: 3.3ms preprocess, 127.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 112.5ms\n","Speed: 3.0ms preprocess, 112.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 109.0ms\n","Speed: 2.2ms preprocess, 109.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 104.6ms\n","Speed: 4.0ms preprocess, 104.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 107.9ms\n","Speed: 2.1ms preprocess, 107.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 113.0ms\n","Speed: 2.4ms preprocess, 113.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 110.1ms\n","Speed: 3.0ms preprocess, 110.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 150.7ms\n","Speed: 2.2ms preprocess, 150.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 174.2ms\n","Speed: 4.0ms preprocess, 174.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 110.2ms\n","Speed: 1.4ms preprocess, 110.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 110.3ms\n","Speed: 5.6ms preprocess, 110.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 108.2ms\n","Speed: 2.0ms preprocess, 108.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 117.9ms\n","Speed: 2.0ms preprocess, 117.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 105.4ms\n","Speed: 2.5ms preprocess, 105.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 113.3ms\n","Speed: 2.2ms preprocess, 113.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 traffic light, 103.8ms\n","Speed: 3.1ms preprocess, 103.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","Video frame is empty or video processing has been successfully completed.\n"]}],"source":["model = YOLO(\"yolov8n.pt\")\n","names = model.model.names\n","\n","cap = cv2.VideoCapture(VIDEO_PATH)\n","assert cap.isOpened(), \"Error reading video file\"\n","w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n","\n","video_writer = cv2.VideoWriter(\"pre_built_yolo_speed_estimation.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n","\n","line_pts = [(0, h//2+80), (w, h//2+80)]\n","\n","speed_obj = speed_estimation.SpeedEstimator()\n","speed_obj.set_args(\n","    reg_pts=line_pts,\n","    names=names,\n","    view_img=True,\n",")\n","\n","while cap.isOpened():\n","    success, frame = cap.read()\n","    if not success:\n","        print(\"Video frame is empty or video processing has been successfully completed.\")\n","        break\n","\n","    tracks = model.track(frame, persist=True, show=False)\n","\n","    frame = speed_obj.estimate_speed(frame, tracks)\n","    video_writer.write(frame)\n","\n","cap.release()\n","video_writer.release()\n","cv2.destroyAllWindows()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":3689772,"sourceId":7185089,"sourceType":"datasetVersion"}],"dockerImageVersionId":30648,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":4}
